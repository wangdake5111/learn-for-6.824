//package raft
//
////
//// this is an outline of the API that raft must expose to
//// the service (or tester). see comments below for
//// each of these functions for more details.
////
//// rf = Make(...)
////   create a new Raft server.
//// rf.Start(command interface{}) (index, Term, isleader)
////   start agreement on a new log entry
//// rf.GetState() (Term, isLeader)
////   ask a Raft for its current Term, and whether it thinks it is leader
//// ApplyMsg
////   each time a new entry is committed to the log, each Raft peer
////   should send an ApplyMsg to the service (or tester)
////   in the same server.
////
//
//import (
//	"fmt"
//	"math/rand"
//	"sync"
//	"time"
//)
//import "labrpc"
//
//// import "bytes"
//// import "labgob"
//
//
//const(
//	FOLLOWER = iota
//	CANDIDATE
//	LEADER
//	MIN_ELECTION_INTERVAL = 450
//	MAX_ELECTION_INTERVAL = 550
//	HEARTB = 50*time.Millisecond
//)
//func randTime(max, min int) time.Duration {
//	return time.Duration(rand.Int()%(max-min)+min)*time.Millisecond
//}
////
//// as each Raft peer becomes aware that Successive log entries are
//// committed, the peer should send an ApplyMsg to the service (or
//// tester) on the same server, via the applyCh passed to Make(). set
//// CommandValid to true to indicate that the ApplyMsg contains a newly
//// committed log entry.
////
//// in Lab 3 you'll want to send other kinds of messages (e.g.,
//// snapshots) on the applyCh; at that point you can add fields to
//// ApplyMsg, but set CommandValid to false for these other uses.
////
//type ApplyMsg struct {
//	CommandValid bool
//	Command      interface{}
//	CommandIndex int
//}
//
////
//// A Go object implementing a single Raft peer.
////
//type Log struct {
//	Term int
//	index int
//	command interface{}
//}
//type Raft struct {
//	mu        sync.Mutex          // Lock to protect shared access to this peer's state
//	peers     []*labrpc.ClientEnd // RPC end points of all peers
//	persister *Persister          // Object to hold this peer's persisted state
//	me        int                 // this peer's index into peers[]
//
//	// Your data here (2A, 2B, 2C).
//	// Look at the paper's Figure 2 for a description of what
//	// state a Raft server must maintain.
//	chanAppend chan bool
//	chanRequest chan bool
//
//	leader int
//	currentTerm int
//	votedFor int
//	voteCount int
//	state int
//	electionTimer *time.Timer
//	//nextIndex []int
//	//matchIndex []int
//	//log []Log
//	//commitIndex int
//	//lastApplied int
//
//}
//
//// return currentTerm and whether this server
//// believes it is the leader.
//func (rf *Raft) GetState() (int, bool) {
//
//	//var Term int
//	//var isleader bool
//
//	// Your code here (2A)
//	// .
//	fmt.Printf("üëºserver %d is a %dÔºåterm is %d\n",rf.me,rf.state,rf.currentTerm)
//	return rf.currentTerm, rf.state==LEADER
//}
//
//
////
//// save Raft's persistent state to stable storage,
//// where it can later be retrieved after a crash and restart.
//// see paper's Figure 2 for a description of what should be persistent.
////
//func (rf *Raft) persist() {
//	// Your code here (2C).
//	// Example:
//	// w := new(bytes.Buffer)
//	// e := labgob.NewEncoder(w)
//	// e.Encode(rf.xxx)
//	// e.Encode(rf.yyy)
//	// data := w.Bytes()
//	// rf.persister.SaveRaftState(data)
//}
//
//
////
//// restore previously persisted state.
////
//func (rf *Raft) readPersist(data []byte) {
//	if data == nil || len(data) < 1 { // bootstrap without any state?
//		return
//	}
//	// Your code here (2C).
//	// Example:
//	// r := bytes.NewBuffer(data)
//	// d := labgob.NewDecoder(r)
//	// var xxx
//	// var yyy
//	// if d.Decode(&xxx) != nil ||
//	//    d.Decode(&yyy) != nil {
//	//   error...
//	// } else {
//	//   rf.xxx = xxx
//	//   rf.yyy = yyy
//	// }
//}
//
//
//// rand[min, max]
//func randInt64InRange(min, max int64) int64 {
//	rand.Seed(rand.Int63()%30)
//	return min + rand.Int63()%(max-min)
//}
//// init or reset timer
//func (rf *Raft) randResetTimer() {
//	if (rf.electionTimer == nil) {
//		rf.electionTimer = time.NewTimer(time.Duration(randInt64InRange(MIN_ELECTION_INTERVAL, MAX_ELECTION_INTERVAL)) * time.Millisecond)
//	} else {
//		rf.electionTimer.Reset(time.Duration(randInt64InRange(MIN_ELECTION_INTERVAL, MAX_ELECTION_INTERVAL)) * time.Millisecond)
//	}
//}
//
////
//// example RequestVote RPC arguments structure.
//// field names must start with capital letters!
////
//type AppendEntriesArgs struct {
//	Term int
//	LeaderId int
//
//	//prevLogIndex int
//	//prevLogTerm int
//	//entries []Log
//	//leaderCommit int
//}
//type AppendEntriesRep struct {
//	Term int
//	Success bool
//}
//
//func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesRep){
//	if(args.Term < rf.currentTerm){
//		reply.Success = false
//		reply.Term = rf.currentTerm
//		return
//	}
//	if(args.Term>rf.currentTerm){
//		rf.currentTerm = args.Term
//		rf.updateState(FOLLOWER)
//	}
//	reply.Success = true
//	reply.Term = rf.currentTerm
//	go func() {
//		DPrintf("server %d(Term = %d) received AppendEntries from LEADER %d(Term = %d)\n",
//			rf.me, rf.currentTerm, args.LeaderId, args.Term)
//		rf.chanAppend <- true
//	}()
//}
//type RequestVoteArgs struct {
//	// Your data here (2A, 2B).
//	Term int
//	CandidateId int
//	//lastLogIndex int
//	//lastLogTerm int
//}
//
////
//// example RequestVote RPC reply structure.
//// field names must start with capital letters!
////
//type RequestVoteReply struct {
//	// Your data here (2A).
//	Term int
//	VoteGranted bool
//}
//
////
//// example RequestVote RPC handler.
////
//func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) {
//	// Your code here (2A, 2B).
//	rf.mu.Lock()
//	if(args.Term<rf.currentTerm) {
//		reply.VoteGranted = false
//		reply.Term = rf.currentTerm
//		rf.mu.Unlock()
//		return
//	}
//	if(args.Term>rf.currentTerm){
//		rf.currentTerm=args.Term
//		rf.mu.Unlock()
//		rf.updateState(FOLLOWER)
//		rf.mu.Lock()
//		defer rf.mu.Unlock()
//		rf.votedFor=args.CandidateId
//	}
//	//if(args.Term>rf.currentTerm) {
//	//	rf.currentTerm = args.Term
//	//	rf.updateState(FOLLOWER)
//	//	rf.votedFor =
//	//}
//	//if (args.Term > rf.currentTerm) {
//	//	rf.currentTerm = args.Term
//	//	rf.updateState(FOLLOWER)
//	//	// ÊòìÈîôÁÇπ
//	//	rf.votedFor = args.CandidateId
//	//}
//	if(rf.votedFor==-1||rf.votedFor==args.CandidateId) {
//		rf.votedFor = args.CandidateId
//		reply.Term = args.Term
//		reply.VoteGranted = true
//		go func() {
//			DPrintf("server %d received RequestVote from CANDIDATE %d, vote for %d\n", rf.me, args.CandidateId, rf.votedFor)
//			rf.chanRequest <- true
//		}()
//		return
//	}else {
//		reply.Term=rf.currentTerm
//		reply.VoteGranted = false
//	}
//
//
//	//if (args.Term < rf.currentTerm) {
//	//	reply.VoteGranted = false
//	//	reply.Term = rf.currentTerm
//	//	return
//	//}
//	//// Êî∂Âà∞‰∫ÜÊñ∞ Term ÁöÑ‰ø°ÊÅØ
//
//	//reply.Term = rf.currentTerm
//	//
//	//if (rf.votedFor == -1 || rf.votedFor == args.CandidateId) {
//	//	// first come first served
//	//	rf.votedFor = args.CandidateId
//	//	reply.VoteGranted = true
//
//
//
//}
//func (rf *Raft) BroadcastAppendEntries(){
//	args := AppendEntriesArgs{Term:rf.currentTerm,LeaderId:rf.leader}
//	for i,_ := range(rf.peers){
//		if(i==rf.me){
//			continue
//		}
//		go func(server int) {
//			var reply AppendEntriesRep
//			if(rf.sendAppendEntriesVote(rf.me, &args, &reply)) {
//				if (reply.Success == true) {
//				} else {
//					if (reply.Term > rf.currentTerm) {
//						rf.mu.Lock()
//						rf.currentTerm = reply.Term
//						rf.updateState(FOLLOWER)
//						rf.mu.Unlock()
//					}
//				}
//			}
//		}(i)
//	}
//}
//func (rf *Raft) BroadcstRequestVotes(){
//	fmt.Printf("server %dÂºÄÂßãÈÄâ‰∏æ\n",rf.me)
//	rf.mu.Lock()
//	rf.voteCount = 1
//	rf.currentTerm +=1
//	fmt.Printf("üòÄserver %dÁöÑcurrentTermÊòØ%d\n",rf.me,rf.currentTerm)
//	rf.votedFor = rf.me
//	rf.randResetTimer()
//	rf.mu.Unlock()
//	args := RequestVoteArgs{Term:rf.currentTerm,CandidateId:rf.me}
//	for i,_ := range(rf.peers){
//		if(i==rf.me){
//			continue
//		}
//		go func(server int) {
//			var reply RequestVoteReply
//			if(rf.sendRequestVote(rf.me, &args, &reply)) {
//				if (reply.VoteGranted == true) {
//					rf.mu.Lock()
//					rf.voteCount += 1
//					rf.mu.Unlock()
//				} else {
//					if (reply.Term > rf.currentTerm) {
//						rf.mu.Lock()
//						rf.currentTerm = reply.Term
//						rf.mu.Unlock()
//						rf.updateState(FOLLOWER)
//					}
//				}
//			}
//		}(i)
//	}
//}
//func (rf *Raft) updateState(state int) {
//	stateDesc := []string{"FOLLOWER", "CANDIDATE", "LEADER"}
//	rf.mu.Lock()
//	preState := rf.state
//	switch state {
//	case FOLLOWER:
//		rf.state = FOLLOWER
//		rf.votedFor = -1 // prepare for next election
//		rf.mu.Unlock()
//	case CANDIDATE:
//		rf.state = CANDIDATE
//		rf.mu.Unlock()
//	case LEADER:
//		if preState==CANDIDATE {
//			rf.state = LEADER
//		}
//		rf.mu.Unlock()
//	default:
//		fmt.Printf("Warning: invalid state %d, do nothing.\n", state)
//	}
//
//	fmt.Printf("In Term %d: Server %d transfer from %s to %s\n",
//		rf.currentTerm, rf.me, stateDesc[preState], stateDesc[rf.state])
//}
//
////
//// example code to send a RequestVote RPC to a server.
//// server is the index of the target server in rf.peers[].
//// expects RPC arguments in args.
//// fills in *reply with RPC reply, so caller should
//// pass &reply.
//// the types of the args and reply passed to Call() must be
//// the same as the types of the arguments declared in the
//// handler function (including whether they are pointers).
////
//// The labrpc package simulates a lossy network, in which servers
//// may be unreachable, and in which requests and replies may be lost.
//// Call() sends a request and waits for a reply. If a reply arrives
//// within a timeout interval, Call() returns true; otherwise
//// Call() returns false. Thus Call() may not return for a while.
//// A false return can be caused by a dead server, a live server that
//// can't be reached, a lost request, or a lost reply.
////
//// Call() is guaranteed to return (perhaps after a delay) *except* if the
//// handler function on the server side does not return.  Thus there
//// is no need to implement your own timeouts around Call().
////
//// look at the comments in ../labrpc/labrpc.go for more details.
////
//// if you're having trouble getting RPC to work, check that you've
//// capitalized all field names in structs passed over RPC, and
//// that the caller passes the address of the reply struct with &, not
//// the struct itself.
////
//func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) bool {
//	ok := rf.peers[server].Call("Raft.RequestVote", args, reply)
//	return ok
//}
//func (rf *Raft) sendAppendEntriesVote(server int, args *AppendEntriesArgs, reply *AppendEntriesRep) bool {
//	ok := rf.peers[server].Call("Raft.AppendEntries", args, reply)
//	return ok
//}
//
////
//// the service using Raft (e.g. a k/v server) wants to start
//// agreement on the next command to be appended to Raft's log. if this
//// server isn't the leader, returns false. otherwise start the
//// agreement and return immediately. there is no guarantee that this
//// command will ever be committed to the Raft log, since the leader
//// may fail or lose an election. even if the Raft instance has been killed,
//// this function should return gracefully.
////
//// the first return value is the index that the command will appear at
//// if it's ever committed. the second return value is the current
//// Term. the third return value is true if this server believes it is
//// the leader.
////
//func (rf *Raft) Start(command interface{}) (int, int, bool) {
//	index := -1
//	Term := -1
//	isLeader := true
//
//	// Your code here (2B).
//
//
//	return index, Term, isLeader
//}
//
////
//// the tester calls Kill() when a Raft instance won't
//// be needed again. you are not required to do anything
//// in Kill(), but it might be convenient to (for example)
//// turn off debug output from this instance.
////
//func (rf *Raft) Kill() {
//	// Your code here, if desired.
//}
//
////
//// the service or tester wants to create a Raft server. the ports
//// of all the Raft servers (including this one) are in peers[]. this
//// server's port is peers[me]. all the servers' peers[] arrays
//// have the same order. persister is a place for this server to
//// save its persistent state, and also initially holds the most
//// recent saved state, if any. applyCh is a channel on which the
//// tester or service expects Raft to send ApplyMsg messages.
//// Make() must return quickly, so it should start goroutines
//// for any long-running work.
////
//func Make(peers []*labrpc.ClientEnd, me int,
//	persister *Persister, applyCh chan ApplyMsg) *Raft {
//	rf := &Raft{}
//	rf.peers = peers
//	rf.persister = persister
//	rf.me = me
//
//	// Your initialization code here (2A, 2B, 2C).
//	rf.currentTerm = 0
//	rf.votedFor = -1
//	rf.state = FOLLOWER
//	rf.voteCount = 0
//	rf.chanAppend = make(chan bool,100)
//	rf.chanRequest = make(chan bool,100)
//	rf.leader = -1
//	rf.randResetTimer()
//	rf.readPersist(persister.ReadRaftState())
//
//	// initialize from state persisted before a crash
//	go func() {
//		for {
//			switch rf.state {
//			case FOLLOWER:
//				select {
//				// ÈòªÂ°ûÁõ¥Âà∞ÂÖ∂‰∏≠‰∏Ä‰∏™ case ÊàêÁ´ã
//				case <-rf.chanAppend:
//					DPrintf("received append request, reset timer for server %d.\n", rf.me)
//					rf.randResetTimer()
//				case <-rf.chanRequest:
//					DPrintf("received vote request, reset timer for server %d.\n", rf.me)
//					rf.randResetTimer()
//				case <-rf.electionTimer.C:
//					// Ë∂ÖÊó∂
//					rf.updateState(CANDIDATE)
//					rf.BroadcstRequestVotes();
//					fmt.Printf("server %d become CANDIDATE, Term = %d\n", rf.me, rf.currentTerm)
//				}
//			case CANDIDATE:
//				select {
//				case <-rf.chanAppend:
//					// ÂÖ∂‰ªñÊúçÂä°Âô®Â∑≤ÁªèÊàê‰∏∫ LEADER
//					DPrintf("server %d become FOLLOWER", rf.me)
//					rf.updateState(FOLLOWER)
//				case <-time.After(300*time.Millisecond):
//					// Ë∂ÖÊó∂ÔºåÊñ∞‰∏ÄËΩÆÈÄâ‰∏æ
//					fmt.Printf("üê≠New Election Started...\n")
//					rf.BroadcstRequestVotes();
//				default:
//					// avoid race
//					// if (rf.voteCount > len(rf.peers)/2) {
//					var win bool
//					rf.mu.Lock()
//					if (rf.voteCount > len(rf.peers)/2) {
//						win = true
//					}
//					rf.mu.Unlock()
//					if (win == true) {
//						// Ëµ¢ÂæóÈÄâ‰∏æ
//						fmt.Printf("üëøserver %d got %d out of %d vote, become LEADER, Term = %d\n",
//							rf.me, rf.voteCount, len(rf.peers), rf.currentTerm)
//						rf.updateState(LEADER)
//						rf.BroadcastAppendEntries()
//						// rf.maintainAuthority()
//					} else {
//						// DPrintf("server %d only got %d out of %d vote ,remain CANDIDATE\n", rf.me, rf.voteCount, len(rf.peers))
//					}
//				}
//			case LEADER:
//				rf.BroadcastAppendEntries()
//				time.Sleep(HEARTB)
//			}
//		}
//	}()
//
//
//
//	return rf
//}
